# Kitoboy
Система помощи в противодействии суициду

# Дата релиза

15.05.2025-03.06.2025

Следите за новостями в нашей [группе в Телеграме](https://t.me/+axSZVk-mLHVkMjEy).

# О системе

Система помогает оценивать уровень суицидального статуса для пользователей социальных сетей по их открытым постам. Для этого в систему загружаются посты со страницы пользователя, затем к каждому посту модели машинного обучения делают предсказания о наличии каких-либо признаков, связанных с суицидальным поведением. 

Есть две главные модели, предсказывающие важные классы признаков. Первый это пресуицидальные признаки (сигналы) — те, что склояют человека к суициду. Второй это антисуицидальыне признаки — те, что удерживают человека. Предсказания моделей позволяют отсеять нерелевантные посты, составляющие до 80 процентов от объема постов, что позволяет волонтерам сосредоточиться на главном. На основе предсказаний система позволяет оценить динамику проявления того или иного признака во времени. Наконец, волонтер может выставить суицидальный статус пользователю социальных сетей.

Система позволяет вести учёт. Имеется возможность создать карточку, которая отражает информацию о реальном человеке, найденную по открытым постам. Поскольку реальный человек может иметь несколько аккаунтов в социальных сетях, к одной карточке может быть присоединено несколько аккаунтов. Также, карточка имеет отдельный суицидальный статус, связанный именно с реальным человеком, а не аккаунтом. 

**Важное замечание**: собирая персональные данные о реальном человеке даже по открытой информации, вы должны иметь право на такую деятельность.

# Технические требования

Для запуска системы вам понадобится машина, у которой минимально выполнены следующие требования:
* Работает на базе операционной системы Linux. Тест запуска проводился на:
  * Ubuntu 18.04
  * Debian 9
* Установлена программа docker-compose версии не ниже 2.29.7.
* Установлена программа Docker версии не ниже 20.10.21.
* Имеется не менее 16 Гигабайт оперативной памяти.
* Установлен процессор Intel семейства Core i7 8 поколения.

# Быстрый старт

## Запуск системы:
1) Подготовить окружение

Для этого в корне репозитория создайте файл окружения `.env` на основе шаблона `.env.example`. Далее в созданном файле необходимо изменить значение переменной `NODE_ENV` на `production`. `DB_NAME` и `DB_HOST` должны остаться без изменения. Остальные переменные можно оставить со значениями по умолчанию, либо изменить на строковые значения. Пояснения к переменным:
* DB_USER — имя пользователя базы данных.
* DB_PASSWORD — пароль пользователя базы данных.
* PG_USER — имя пользователя в PGAdmin.
* PG_PASSWORD — пароль пользователя в PGAdmin.
* UI_USER — имя пользователя платформы.
* UI_PASSWORD — пароль пользователя платформы.
* AUTH_KEY — секретный ключ для авторизации с использованием JWT-токена.
* COMPOSE_FILE — исходный docker-compose файл для развертывания системы.

2) Собрать образы Docker:

Для этого выполните следующую команду:
```
$ docker-compose build --no-cache
```
3) Запустить контейнеры в фоновом режиме:
 
Для этого выполните команду:
```
$ docker-compose up --build -d
```

После успешного запуска система будет доступна по адресу: http://localhost:5173/kitoboy. Логин и пароль будет совпадать с теми, что были указаны в UI_USER и UI_PASSWORD

Взаимодействие с системой детально описано в [pdf-руководстве пользователя](./docs/user_guide.pdf).

### Данные для обработки: 
Наполнение системы данными происходит на основе загрузки файлов следующего формата:

- Формат: CSV
- Заголовки колонок отсутствуют - данные парсятся, начиная с первой строки
- Первая колонка - дата и время с тайм-зоной в формате UTC (напр. `2025-04-10T12:38:22.922Z`)
- Вторая колонка - текст

Пример файла можно найти в [репозитории](./docs/input_file_example.csv).
Остальные данные заполняются вручную через форму.

# Разработка

Для удобной разработки ниже описан способ запуска системы в режиме, когда активен Hot Module Reload (HMR) в сервисах Frontend и API.

1) Подготовить окружение

Подготовка идентична той, что указана в разделе "Запуск системы", за исключением того, что значение переменной `NODE_ENV` должно быть установлено в `development`.

2) Установить зависимости для Frontend:

Для этого выполните следующую команду:
```
$ cd frontend
$ npm i
```
3) Установить зависимости для API:

Для этого выполните следующие команды:
```
$ cd ../api
$ npm i
$ cd ..
```

4) Собрать образы Docker:

Для этого выполните следующую команду:
```
$ docker-compose build --no-cache
```
5) Запустить контейнеры:

Для этого выполните следующую команду:
```
$ docker-compose up --build
```

6) Запустить Frontend в контейнере:
```
$ docker-compose exec frontend sh
$ npm run dev 
```
Выход
```
$ cmd+c
$ exit  
```
Описание API библиотеки доступно в [pdf-руководстве разработчика](./docs/developer_guide.pdf).

# Перечень направлений использования библиотеки

1.  Анализ пользователей социальных сетей на предмет суицидального поведения на основе их текстовых публикаций с возможностью вынесения объяснимого решения о суицидальном статусе пользователя.
2.  Отслеживание динамики постов пресуицидального и антисуицидального характера у пользователей социальных сетей.
3.  Сбор и анализ персональной информации о пользователе на основе его текстов в социальных сетях.
4.  В общем случае, анализ серии текстовых публикаций человека при наличии соответствующей модели (эмоции, патологических психических состояний, степени удовлетворенности товаром и т. д.)

# Как еще можно использовать библиотеку

## Использование модуля "Зоопарк" в качестве бекэнда  для чатботов

Модуль "Зоопарк" (zoo) предоставляет единый интерфейс для инференса многих бертоподобных моделей классификации. Именно он ответственен за наполнение Китобоая предсказаниями моделей. Его можно использовать как самостоятельный компонент в других приложениях.

Здесь мы покажем, как мы используем Зоопарк вместе с ботом мессенджера Телеграм, который использовался для демонстрационных целей.

Далее предполагается, что вы будете использовать модели пресуицидальных и антисуицидальных сигналов. Также предполагается, что развертывание будет происходит на локальной машине и все необходимые зависимости установлены.

1. В случае если система «Китобой» работает её необходимо выключить, находясь в корне проекта, с помощью команды:
```
docker-compose down
```
Также можно поменять порты запуска контейнров (см. далее)
2. Запустить контейнеры с моделями
 
```
$ docker run -p 8000:8000 astromis/tritoned_presui_model-15:v1
$ docker run -p 8001:8001 astromis/tritoned_antisui_model-10:v1
```
Если указанные порты заняты (либо системой «Китобой», либо другим приложением), их следует заменить на свободные.

3. Указать в конфигурационном файле `./zoo/config/triton_services.yml` местонахождение моделей

```yaml
- ["http://localhost", "8000", "presui_model-15"]
- ["http://localhost", "8001", "antisui_model-10"]
```

4. В конфигурационном файле `./zoo/config/config.yml` установить значение `endpoint_to_send_preds` в `null`.

5. Создать окружение в корне проекта
```
$ python3 -m venv venv
$ source venv/bin/activate
$ pip install -r zoo/requirements.txt
```

6. Запустить Зоопарк командой (если необходимо смотреть за логами, а также иметь возможность автоперезагрузки сервера, то заменить `run` на `dev`)
```
$ cd zoo && fastapi run app/main.py --port=8888
```
7. Запустить контейнер с базой данных
```
$ docker run --name kitoboy_db -e POSTGRES_USER=kitoboy_user -e POSTGRES_PASSWORD=mysecretpassword -e POSTGRES_DB=kitoboy_db -p8080:5432 -d postgres:15-alpine
```
8. Установить в окружении зависимости бота 
```
$ pip install -r telegram_bot/requirements.txt
```
9. Указать в файле `./telegram_bot/create_table.py` параметры базы данных (как минимум, пароль) в словаре на строчке 3.
10. Создать (проверить наличие) таблицу командой
```
$ python telegram_bot/create_table.py
```
11.  Указать в файле `./telegram_bot/bot_aiogram.py` токен бота в строчке 155, а также параметры базы данных (как минимум, пароль) в словаре на строчке 157
12. Запустить бота командой
```
$ python telegram_bot/bot_aiogram.py
```

После запуска в бот можно отсылать тесты, а бот будет присылать предсказанные сигналы с предложением оценить верность предсказания.

## Использование открытой библиотеки для анализа эмоционального состояния и динамики пользователей 

Систему можно использовать для анализа любых феноменов, поддающихся классификации. В это подразделе показано, как превратить систему в анализатор эмоций. За основу возьмем открытую [модель эмоций](cointegrated/rubert-tiny2-cedr-emotion-detection), обученную на датасете CEDR.


1. Клонируйте репозиторий `Tritoned Bert` в отдельную директорию, вне репозитория `Kitoboy`, и перейдите в сам репозиторий:
```
$ git clone https://github.com/psytechlab/tritoned_bert.git
$ cd tritoned_bert
```

2. Создайте и активируйте виртуальное окружение, затем установите необходимые зависимости:

```
# Создание виртуального окружения
python -m venv .venv
source .venv/bin/activate

# Установка Python-зависимостей
pip install -r requirements.txt

# Установка системных зависимостей
sudo apt-get update
sudo apt-get install -y gettext-base
```

3. В корневой директории репозитория создаем файл `id2label.json` следующего содержания
```json
{
    "0": "no_emotion", 
    "1": "joy", 
    "2": "sadness", 
    "3": "surprise", 
    "4": "fear",
    "5": "anger"
}
```
4. Выполняем команду для конвертации модели и создания Docker-образа:
```
$ ./make_triton_image.sh cointegrated/rubert-tiny2-cedr-emotion-detection cointegrated/rubert-tiny2-cedr-emotion-detection ./id2label.json cedr_emotion_model v1
```

5. Убедитесь, что у вас появился Docker-образ `tritoned_cedr_emotion_model:v1`. В выводе должен присутствовать образ `tritoned_cedr_emotion_model:v1`.
6. Скопируйте или используйте существующий файл `docker-compose.models.production.yml` и модифицируйте его, добавив в уровень `services` (для простоты можно удалить модели presui, antisui, pie):
```yml
# Контейнер с классификатором эмоций
cedr_emotion:
    image: tritoned_cedr_emotion_model:v1
    ports:
        - '5903:8000'
    healthcheck:
        test: ["CMD-SHELL", "curl -f http://localhost:8000/v2/health/ready | exit 1"]
        interval: 10s
        timeout: 5s
        retries: 20
    networks:
        - postgres
```
7. В том же yaml-файле уберите все пункты, кроме `api`, из `services.zoo.depends_on` и добавьте пункт `cedr_emotion`.
8. Сохраните файл.
9. Запустите систем «Китобой», как это указано в разделе «Быстрый старт».

Таким образом, система позволит определять эмоции пользовательских текстов и анализировать динамику эмоционального состояния.

## Адаптация и расширение функционала модуля поиска идентификационной информации

Модуль поиска идентификационной информации является составной частью библиотеки и может быть легко расширен за счет предоставляемого класса `AbstractIE`. В качесиве примера разберем, как можно интегрировать большие языковые модели (БЯМ) в модуль.

Чтобы создать поисковик, необходимо отнаследоваться от класса `AbstractIE` и реализовать метод `make_prediction`. Допустим, имеется сервис `http://llm.ru`, который предоставляет доступ к БЯМ `llama-4-maverick` по OpenAI API. Было решено с помощью БЯМ искать любые упоминания данных российского паспорта. Тогда реализация класса БЯМ могла бы выглядеть следующим образом

```python
import requests
import json

class LlmIE(AbstractIE):

    def make_prediction(self, text: str) -> list[str]:
        json_data = {
            'model': "llama-4-maverick",
            'messages': [
                {
                    'role': 'system',
                    'content': "Определи, есть ли в тексте данные российского паспорта. В случае, если такие данные есть, напиши 'PASPORT'",
                },
                {
                    'role': 'user',
                    'content': text,
                },
            ],
        }
        response = requests.post("http://llm.ru//v1/chat/completions", 
                                headers= {'Content-Type': 'application/json','Authorization': f"Bearer TOKEN",},
                                json=json_data)
        return [json.loads(response_raw.text)["choices"][0]["message"]["content"]]
```

Для простоты можно сохранить этот класс в `./app/information_extraction.py` в конце.

После сохранения в файле `./app/main.py` необходимо импортировать новый класс и инициализировать его объект
```diff
+ from app.information_extraction import NavecIE, RegexIE, LlmIE

app = FastAPI()
app.state.ie_list = [
    NavecIE("data/navec_news_v1_1B_250K_300d_100q.tar", "data/slovnet_ner_news_v1.tar"),
    RegexIE("configs/regex_ie.yml"),
+   LlmIE()
]

```
Таким образом, после запуска сервиса для каждого текста будет использоваться БЯМ для поиска паспортных данных в текстах.
 
